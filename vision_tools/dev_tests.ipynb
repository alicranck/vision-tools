{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abecee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import yaml\n",
    "from PIL.Image import Image as PILImage\n",
    "import cv2\n",
    "\n",
    "# sys.path.append(\"/home/alicranck/almog/projects/vision-tools/vision_tools\")\n",
    "\n",
    "from vision_tools.core.tools.detection import OpenVocabularyDetector\n",
    "from vision_tools.core.tools.captioning import Captioner\n",
    "from vision_tools.core.tools.embedder import CLIPEmbedder, JinaEmbedder\n",
    "from vision_tools.core.tools.base_tool import BaseVisionTool\n",
    "from vision_tools.engine.video_engine import VideoInferenceEngine\n",
    "from vision_tools.core.tools.pipeline import VisionPipeline, PipelineConfig\n",
    "from vision_tools.utils.image_utils import base64_encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a5260",
   "metadata": {},
   "source": [
    "## Test tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870f666",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513923f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_video_url = \"https://cdn.pixabay.com/video/2020/11/13/56310-479197605_large.mp4\"\n",
    "\n",
    "\n",
    "def time_run(tool: BaseVisionTool, n_frames: int):\n",
    "    \n",
    "    \n",
    "    cap = cv2.VideoCapture(demo_video_url)\n",
    "\n",
    "    processed_frames = 0\n",
    "    times = []\n",
    "    while cap.isOpened() and processed_frames < n_frames:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = tool.process(frame, {})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        processed_frames += 1\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    print(f\"Average time: {sum(times[2:]) / len(times[2:])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cad138",
   "metadata": {},
   "source": [
    "### Detector / Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"/home/alicranck/almog/projects/vision-tools/vision_tools/core/configs/ov_detection.yaml\"\n",
    "with open(cfg_path, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "cfg[\"vocabulary\"] = [\"person\", \"car\", \"bus\"]\n",
    "\n",
    "detector = OpenVocabularyDetector(cfg['model'], cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7ac05",
   "metadata": {},
   "source": [
    "### Captioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = LlamaCppCaptioner(\"ggml-org/SmolVLM2-256M-Video-Instruct-GGUF:Q8_0\", {\"imgsz\": 480})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11907b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_run(captioner, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606732ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_box(boxes: list) -> list:\n",
    "    xyxy_boxes = np.array([box.xyxy.cpu().tolist() for box in boxes])\n",
    "    diffs = np.diff(xyxy_boxes, axis=0)\n",
    "    mean_diff = np.ma.average(diffs, axis=0, \n",
    "                        weights=range(len(diffs)))\n",
    "    next_xyxy_box = xyxy_boxes[-1] + mean_diff\n",
    "    return next_xyxy_box.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904356a1",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_tools.core.tools.embedder import SigLIP2Embedder\n",
    "\n",
    "\n",
    "embedder = SigLIP2Embedder(model_id=\"google/siglip2-base-patch16-384\", config={}, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf71644",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/alicranck/Downloads/download.jpeg\"\n",
    "image_embedding = embedder.process(image_path, {}, None)[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = embedder.encode_text(\"a red car\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e634eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cosine_similarity(np.array(image_embedding).reshape(1, -1), np.array(text_embedding).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf123d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/alicranck/Downloads/download.jpeg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "inputs = processor(images=[image], return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    image_embeddings = image_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c6d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"a green field\", \"a red car\", \"a black cat on a red couch\", \"a potato\", \"black shoes\"]\n",
    "tokens = tokenizer(texts, padding=\"max_length\",\n",
    "                    max_length=64, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.get_text_features(**tokens)\n",
    "\n",
    "text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "text_embeddings = text_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24945b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "similarities = cosine_similarity(image_embeddings, text_embeddings)\n",
    "\n",
    "print(\"Semantic Search Results (Cosine Similarity):\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Score: {similarities[0][i]:.4f} | Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf29de0",
   "metadata": {},
   "source": [
    "## Video engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117858c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = PipelineConfig(\n",
    "    tool_settings={\n",
    "        \"embedding\": {\n",
    "            \"trigger\": {\"type\": \"stride\", \"value\": 150}\n",
    "        },\n",
    "        \"ov_detection\": {\n",
    "            \"vocabulary\": [\"person\", \"car\", \"dog\", \"cat\", \"chair\"],\n",
    "            \"trigger\": {\"type\": \"stride\", \"value\": 150}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline = VisionPipeline(pipeline_config)\n",
    "engine = VideoInferenceEngine(pipeline, \"/home/alicranck/Downloads/הכנסה.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16af9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _persist_data(data):\n",
    "    tools_run = data['tools_run']\n",
    "    if not tools_run:\n",
    "        return\n",
    "    \n",
    "    timestamp = data['metadata']['timestamp']\n",
    "    metadata = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"data\": data[\"embedding\"]\n",
    "    }\n",
    "\n",
    "    print(metadata)\n",
    "    \n",
    "\n",
    "\n",
    "# Run engine\n",
    "async for _ in engine.run_inference(\n",
    "    on_data=_persist_data, \n",
    "    buffer_delay=0, \n",
    "    realtime=False\n",
    "):\n",
    "    pass\n",
    "\n",
    "# Cleanup\n",
    "pipeline.unload_tools()\n",
    "logger.info(f\"Finished indexing {video_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
